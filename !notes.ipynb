{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77f45e9-622b-42f5-946c-d1eb3b7f2250",
   "metadata": {},
   "source": [
    "# General notes about things I've learned, not lesson specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83943d14-f3c7-41c1-8d9c-22a5417f7b34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How to install a python package for dev, or latest from git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea94056-af6b-4213-94f4-ca36e5eadbe4",
   "metadata": {},
   "source": [
    "Clone the repo, the cd into it and:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab945c-8088-41a0-8c88-7eda2166b0c1",
   "metadata": {},
   "source": [
    "    pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93d7343-d408-44dc-ae77-0e9959ff834c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c909f-e916-4e09-8c0e-de6c085022e8",
   "metadata": {},
   "source": [
    "### Diff two notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d03fd-c6eb-49a6-96aa-d424f15ccfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nbdime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cba03a-3c8c-44d8-9b5e-03dd3446b54a",
   "metadata": {
    "tags": []
   },
   "source": [
    "[nbdime](https://nbdime.readthedocs.io/en/latest/) is recommended [here on stackoverflow](https://stackoverflow.com/questions/18171968/is-there-any-way-to-generate-a-diff-between-two-versions-of-an-ipython-notebook).\n",
    "\n",
    "I don't recommend to run `nbdiff-web` from a notebook, it hangs the notebook, might have to kill it from a terminal!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bba96-192a-4618-b0f4-c5d175149f95",
   "metadata": {},
   "source": [
    "    nbdiff-web --ignore-outputs notes.ipynb 11_notes.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa534d4-ae85-430f-9ac7-9b9b72a57aff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trust a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7dd98-9892-42aa-a257-c1deffe29688",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter trust notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb1d892-0800-4df7-9f62-3b4ba84d7dff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8e13d6-a725-4f72-bb46-b30c683efb7a",
   "metadata": {},
   "source": [
    "Of course I still don't know much about git after more than 13 years...!\n",
    "\n",
    "Maybe I should RTFM and actually learn it with Anki or something."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d2903-077d-42ea-9abe-7ddf1ad9aecc",
   "metadata": {},
   "source": [
    "Piping color git output to less:    \n",
    "\n",
    "    git -c color.status=always status | less -R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6831ccc-4a5f-44c4-b149-7039ee11a2ec",
   "metadata": {},
   "source": [
    "Unstage something from git:\n",
    "    \n",
    "    git reset -- file dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41c4e8-f79e-4037-a282-2f05a913d445",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9ba8b-431f-4f93-bea0-66f6205e6d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5f2c6-7b0d-4dbd-a117-2145c64a584f",
   "metadata": {},
   "source": [
    "\\#discussions\n",
    "\n",
    "[6:24 PM]Nike-Zoldyck: Hey Everyone, I'm curious about what your usual fix is when you run into a CUDA out of memory error, other than lowering batch size . I've set mine to 1 on a GNN model that worked fine before with batch size 5 on similar GPU VM, but i had to replace the machine and create everything from scratch. Doing export PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:128  didn't help either. \n",
    "\n",
    " â€” 11/02/2022\n",
    "\n",
    "[1:38 AM]Alexey Zaytsev: In the end, the only way to fix the issue is to find out what's using the memory. To be honest, I don't know if there is a solid way to find that out after the OOM happened - I had cases where no large tensor seems to exist among python variables, but the GPU memory is not freed, despite attempts at gc.collect() and empty_cache().  Would love to hear if someone has a more systematic way to debug PyTorch CUDA memory issues.\n",
    "\n",
    "[1:50 PM]satan_99: After reducing the batch size, you can use gradient accumulation.\n",
    "\n",
    "[2:18 PM]Nike-Zoldyck: I was able to figure it out after some trial and error and trying out all methods.  The loss, preds and targets were all on gpu too. When logging them or performing metrics calculation, they take up a lot of space. Moving them to cpu without detaching them from the computation graph helped fix it and I was able to increase the batch size and run too\n",
    "\n",
    "[2:19 PM]Nike-Zoldyck: Yeah this didn't work even when the batch size was 1\n",
    "\n",
    "[2:19 PM]Alexey Zaytsev: Why not detach them before moving?\n",
    "\n",
    "[2:19 PM]Nike-Zoldyck: Then back prop won't work and your loss with be a constant value after epoch 1, and nothing will train\n",
    "\n",
    "[2:20 PM]Alexey Zaytsev: Buttt, you leave the loss on gpu and make a copy with .cpu().\n",
    "\n",
    "[2:20 PM]Alexey Zaytsev: loss = loss_f(y, hat); log_save(loss.cpu()); loss.back() \n",
    "\n",
    "[2:21 PM]Nike-Zoldyck: You can use loss as loss.item() instead of the whole tensor and while losing you can do loss.cpu().  For metrics, you can pass preds and targets but do a detach().cpu()\n",
    "\n",
    "[2:21 PM]Nike-Zoldyck: Yes that's what I meant\n",
    "\n",
    "[2:22 PM]Alexey Zaytsev: Sorry, I meant. \n",
    "\n",
    "[2:22 PM]Alexey Zaytsev: log(loss.detach().cpu())\n",
    "\n",
    "[2:24 PM]Nike-Zoldyck: I've faced error using detach VS just cpu\n",
    "\n",
    "[2:24 PM]Nike-Zoldyck: Detaching will remove from computation graphs so there's no way backward\n",
    "\n",
    "[2:24 PM]Alexey Zaytsev: It makes a copy, original loss  is still attached.\n",
    "\n",
    "[2:30 PM]Alexey Zaytsev: More importantly, the gradient is back-propagated between devices, .cpu() or .cuda() does not break the link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11b216-b7f9-44a2-9741-03cdf8facec6",
   "metadata": {},
   "source": [
    "<!-- ![](https://cdn.discordapp.com/attachments/766837559920951316/1037206929694523402/unknown.png) -->\n",
    "![](notes/cuda1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd4d5a-1700-46c5-8b35-49d618ab2e95",
   "metadata": {},
   "source": [
    "[2:36 PM]Alexey Zaytsev: I'm not sure how it works with reference counting. If if del t, I can still call .backward() on tc.sum(), so I assume it just keeps a reference to t and t is not cleared from CUDA memory. \n",
    "\n",
    "[2:44 PM]Nike-Zoldyck: yeah probably, which ends up taking a lot of space over batches and before the epoch fully completes and everything is reset. so reducing batch size won't matter since it is not cleared or reset until the epoch finishes and there are multiple copies of the same variables\n",
    "\n",
    "[2:46 PM]Alexey Zaytsev: Exactly. Which is why it needs to be detached. .item() does it internally.\n",
    "\n",
    "[2:47 PM]Alexey Zaytsev:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eee998-fc2d-4b5e-9e81-205ad955b0cd",
   "metadata": {},
   "source": [
    "<!-- ![](https://cdn.discordapp.com/attachments/766837559920951316/1037211289199595620/unknown.png) -->\n",
    "![](notes/cuda2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82593008-50a5-4dc3-9e87-981f5a588240",
   "metadata": {},
   "source": [
    "[3:20 PM]Nike-Zoldyck: hmm, i've read that these two methods might release the memory but not to Python\n",
    "\n",
    "[3:21 PM]Nike-Zoldyck: so you're saying if we detach it, it doesn't release the cache because it didn't keep the memory in the first place? \n",
    "\n",
    "[3:23 PM]Alexey Zaytsev: No, I'm saying, if you call .cpu() without .detach(), the resulting tensor holds a reference to the tensor that still lives in GPU memory (because .cpu() does not move, but copies a tensor), and that GPU tensor can't be freed until you free the CPU copy of the tensor. \n",
    "\n",
    "[3:24 PM]Alexey Zaytsev: Of course, only applies to tensors that have requires_grad=True.\n",
    "\n",
    "[3:24 PM]Nike-Zoldyck: Oh , that makes sense !\n",
    "\n",
    "[3:35 PM]Alexey Zaytsev: Another source of annoying memory leaks I just found - ipython  keeps all cell outputs in _<n> variables for all previous cells executed. So if your tensor is the cell output (part of the final expression), it won't be freed, because a reference to it is held by ipython.\n",
    "\n",
    "[3:38 PM]Alexey Zaytsev:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee9218-4c6c-4a2a-b6fe-8c4a22eff958",
   "metadata": {},
   "source": [
    "<!-- ![](https://cdn.discordapp.com/attachments/766837559920951316/1037224105549766666/unknown.png) -->\n",
    "![](notes/cuda3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0300331-28aa-4947-b4e7-8299d215a7bb",
   "metadata": {},
   "source": [
    "[3:40 PM]Alexey Zaytsev: I feel this is actually a big deal, and the source of the weird CUDA leak I mention in the beginning, where something holds references to variables that you assume are gone.\n",
    "\n",
    "[3:41 PM]Alexey Zaytsev: The solution is probably to pass --InteractiveShell.cache_size=0 to ipython.\n",
    "\n",
    "[3:56 PM]Alexey Zaytsev: And you can't even use %config InteractiveShell.cache_size=0, you have to put it into the ipython config.\n",
    "\n",
    "[3:56 PM]Alexey Zaytsev: Yes, ðŸ¤¯\n",
    "\n",
    "[3:57 PM]Nike-Zoldyck: I've never run into this issue on notebooks tho. Always when running scripts on terminal. those fixes probably won't work for the regular case right?\n",
    "\n",
    "[3:59 PM]Alexey Zaytsev: Yes, this only applies to notebooks and ipython in general.\n",
    "\n",
    "[4:06 PM]Alexey Zaytsev: @jeremyhoward , did you know about this? I feel it's a pretty significant source of suffering when using notebooks and dealing with weird CUDA OOMs and more people should be aware of it.\n",
    "\n",
    "[5:00 PM]jeremyhoward: in theory, yes i knew about it. in practice, i haven't considered it nearly as much as i should have\n",
    "\n",
    "[5:02 PM]jeremyhoward: btw for a bit less typing i think you could use .data instead of .detach()\n",
    "\n",
    "[5:03 PM]jeremyhoward: also fastai's to_np() does it for you\n",
    "\n",
    "[5:05 PM]jeremyhoward: apparently this is meant to work %config ZMQInteractiveShell.cache_size = 0\n",
    "\n",
    "[5:06 PM]jeremyhoward: %reset -f out is meant to remove all stuff in the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc6f21-180d-4b74-8751-a9333d166b65",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00efe92-4cd4-4145-bf55-0f36b449ea6d",
   "metadata": {},
   "source": [
    "- a fast.ai model.pkl file is actually a zip file containing archive dir, with archive/data.pkl file and other data files\n",
    "- I can do two prompts at once using diffusers on my 2060S (8GB) now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11419a13-1f61-482a-a6fa-61b7c52a9e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
