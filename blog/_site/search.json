[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sam’s AI Blog",
    "section": "",
    "text": "My setup for AI work on my Debian PC\n\n\n\n\n\n\n\nsetup\n\n\nDebian\n\n\nfastai\n\n\nPython\n\n\nCUDA\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\n\n\n\n\n\n\nGraveyard of bad setup ideas\n\n\n\n\n\n\n\nbad\n\n\nsetup\n\n\nDebian\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\n\n\n\n\n\n\nKeyboard shortcuts for Jupyter Lab\n\n\n\n\n\n\n\nsetup\n\n\nJupyter\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n\n\nStartup script for my Linux desktop\n\n\n\n\n\n\n\nbad\n\n\nsetup\n\n\nDebian\n\n\ntools\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n\n\nBlogging AI with Jupyter and Quarto\n\n\n\n\n\n\n\nsetup\n\n\nblog\n\n\nJupyter\n\n\nQuarto\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Sam Watkins, an AI enthusiast based in Melbourne, Australia. I come from a background in computer science and I am interested in helping people to learn using technology. I believe AI has the potential to make a positive impact on society if it doesn’t annihilate us all by accident.\nIn this blog, I’ll be sharing my notes and insights from studying the fast.ai book and courses. My goal is to document my learning journey and provide a resource for others who are interested in learning about AI. Whether you’re a beginner or an experienced practitioner, you’ll likely be horrified, confused or overwhelmed if you read any of my blog posts! But maybe you’ll find something useful here.\nI’m active on various social networks, so feel free to connect with me on any of these platforms."
  },
  {
    "objectID": "posts/shortcuts/shortcuts.html",
    "href": "posts/shortcuts/shortcuts.html",
    "title": "Keyboard shortcuts for Jupyter Lab",
    "section": "",
    "text": "Keyboard shortcuts are a great way to speed up your workflow in Jupyter Lab, but sometimes the default shortcuts don’t quite fit your needs. In this short post, I’ll show you how to add four new keyboard shortcuts to Jupyter Lab that I find useful.\n\n\n\nshortcut\nfunction\n\n\n\n\nCtrl Shift Enter\nRun All Above Selected Cell\n\n\nAlt L\nClear All Outputs\n\n\nAlt M\nRender All Markdown Cells\n\n\nShift V\nPaste Cells Above"
  },
  {
    "objectID": "posts/shortcuts/shortcuts.html#editing-the-settings",
    "href": "posts/shortcuts/shortcuts.html#editing-the-settings",
    "title": "Keyboard shortcuts for Jupyter Lab",
    "section": "Editing the settings",
    "text": "Editing the settings\nGo to Settings → Advanced Settings Editor → JSON Settings Editor\nThen add these shortcuts at the end of the file.\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-all-above\",\n            \"keys\": [\n                \"Ctrl Shift Enter\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:clear-all-cell-outputs\",\n            \"keys\": [\n                \"Alt L\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:render-all-markdown\",\n            \"keys\": [\n                \"Alt M\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:paste-cell-above\",\n            \"keys\": [\n                \"Shift V\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        }\nRemember to add a trailing comma on the previous entry, and no comma after the final entry."
  },
  {
    "objectID": "posts/shortcuts/shortcuts.html#conclusion",
    "href": "posts/shortcuts/shortcuts.html#conclusion",
    "title": "Keyboard shortcuts for Jupyter Lab",
    "section": "Conclusion",
    "text": "Conclusion\nWith these new keyboard shortcuts, you can quickly run all the cells above your current selection, clear all cell outputs, render all markdown cells, and paste cells above your current selection. By customizing your shortcuts to fit your needs, you can save time and increase your productivity in Jupyter Lab."
  },
  {
    "objectID": "posts/startup/startup.html",
    "href": "posts/startup/startup.html",
    "title": "Startup script for my Linux desktop",
    "section": "",
    "text": "This is my startup.sh script, to open various apps and webpages when I log in to my Linux PC, and arrange them on my workspaces.\nTesting and debugging this script is a bit annoying. My previous approach was to make fixes to the script, close everything, exit my desktop session, then log in again and see what happens. Another possibility is to copy-paste lines out of the script into the terminal.\nNow I’m going to try developing the script as a Jupyter notebook with the Bash kernel. I will also generate the shell script from the notebook.\nMy startup script calls out to several other scripts, which aren’t included in this blog post yet."
  },
  {
    "objectID": "posts/startup/startup.html#workspaces-and-apps",
    "href": "posts/startup/startup.html#workspaces-and-apps",
    "title": "Startup script for my Linux desktop",
    "section": "Workspaces and Apps",
    "text": "Workspaces and Apps\nI use two monitors:\n\nLeft monitor, doesn’t switch workspaces, for apps that I want always to hand.\nRight monitor, widescreen, does switch between workspaces, for work.\n\nApps on left monitor:\n\nGnome Pomodoro - timer\nTrello - tasks planner\nChatGPT - AI assistant\nMiniflux - blog reader\nAnki - learning with flashcards\nSwatch - my system monitor script\n\nI am currently using these workspaces:\n\nEntertainment\n\nplaying music\nwatching movies\nxterm with GNU screen\n\nChat and Social\n\nDiscord\nSlack\n\nMiscellaneous\n\nsysadmin\nrandom stuff\n\nAI Study\n\nJupyter notebook\nQuarto blog\nxterm with GNU screen\n\n\nI can add extra workspaces as needed when I work on other projects."
  },
  {
    "objectID": "posts/startup/startup.html#the-startup-script",
    "href": "posts/startup/startup.html#the-startup-script",
    "title": "Startup script for my Linux desktop",
    "section": "The startup script",
    "text": "The startup script\n\n#!/bin/bash\n# usage: source from ~/.xsessionrc\n\n\n# set Right Alt to be the compose key, so I can type — em-dashes and such!\nsetxkbmap -option compose:ralt\n\n\nsleep 1\n\n\ngnome-dark\nconnects &\n\n\nwmctrl -s 0\n\n\nCommon apps on the left monitor\n\nchrome_profile Sam --new-window \\\n\"https://trello.com/b/dnuAgKw1/ai\" \\\n\"https://trello.com/b/y16reUe3/misc\" \\\n\"https://chat.openai.com/chat\" \\\n\"https://chat.openai.com/chat\" \\\n\"https://miniflux.ucm.dev/unread\" \\\n\"https://www.youtube.com/playlist?list=PLqkzs79RU9yUrYAN083Jbiza9DMqQE1dq\" \\\n&\n\n\ngnome-pomodoro &\nbarrier &\nanki &\nswatch &\n# flameshot &\n# miniflux &\n# chatgpt &\n\n\n# Sleep for a bit, and try to get out of the Activities Overview\nsleep 5; xdotool key 'Super'; sleep 1 ; xdotool key 'Escape'\n\n\nwmctrl -r Trello -e 0,960,0,960,1080\nwmctrl -r pomodoro -e 0,0,0,400,400\nwmctrl -r Barrier -e 0,400,0,400,400\nwmctrl -r Anki -e 0,0,0,960,1080\nxdotool search --name swatch windowmove 600 100\n\nfor app in Trello pomodoro Anki swatch Trello Miniflux; do\n    wmctrl -r \"$app\" -b add,sticky\ndone\nwmctrl -r pomodoro -b add,above\nwmctrl -r swatch -b add,above\n\nfor app in barrier; do wmctrl -c $app; done\nfor app in swatch pomodoro; do xdotool search --name $app windowminimize %@; done\n\n\ngnome-pomodoro --start\n\n\n\nApps on Workspace 1: Entertainment\n\nxterm -e '. entertain' &\n\n\n\nApps on Workspace 2: Chat and Social\n\nsleep 2\nwmctrl -s 1\n\n\nslack & # --startup &\ndiscord & # --start-minimized &\nchrome_profile Sam --new-window & sleep 1 ; xtile -w `xactivewindow` /2 1 1\nsleep 5\n\n\nwmctrl -r Slack -e 0,0,0,960,1080\nwmctrl -r Discord -e 0,960,0,960,1080\n\n\n\nApps on Workspace 2: Miscellaneous\n\nsleep 2\nwmctrl -s 2\n\n\nchrome_profile Sam --new-window & sleep 1 ; xtile -w `xactivewindow` /2 1 1\n\n\nsleep 2\nnautilus ~/plan.dev & sleep 1; wmctrl -r plan.dev -e 0,3640,0,400,400\n\n\nsw misc sh\nxterm -e \"stty -ixon; xtile /2+1 1 1 ; sxw misc sh ; exec $SHELL\" & sleep 2\nsstuff misc sh \"^L\"\n\n\n\nApps on Workspace 3: AI Study\n\nsleep 2\nwmctrl -s 3\n\n\nzotero &\nxterm -e '. ai' &\n\n\nsleep 2\nxdotool search --name zotero windowminimize %@"
  },
  {
    "objectID": "posts/startup/startup.html#exporting-the-shell-script",
    "href": "posts/startup/startup.html#exporting-the-shell-script",
    "title": "Startup script for my Linux desktop",
    "section": "Exporting the shell script",
    "text": "Exporting the shell script\nI might put this in a separate script or tool later; as it is, I can run it from the notebook.\n\n# __END__\n\n\njupyter nbconvert --no-prompt --to script startup.ipynb --stdout \\\n--TemplateExporter.exclude_raw=True |\nsed -n '/^# __END__/q; p' > startup.sh\nchmod +x startup.sh\n\n[NbConvertApp] Converting notebook startup.ipynb to script\n\n\n\nmv -v ./startup.sh ~/local/x/startup.sh\n\nrenamed './startup.sh' -> '/home/sam/local/x/startup.sh'"
  },
  {
    "objectID": "posts/startup/startup.html#logging-out-to-test-the-script",
    "href": "posts/startup/startup.html#logging-out-to-test-the-script",
    "title": "Startup script for my Linux desktop",
    "section": "Logging out to test the script",
    "text": "Logging out to test the script\nThis notebook can helpfully kill itself, an unusual characteristic for a notebook!\n\nxdotool search --name 'Chrome' windowclose %@\nsleep 1\nkillall screen Xorg\n\n\n\nSee also\n\n\nLifeline Australia"
  },
  {
    "objectID": "posts/startup/startup.html#conclusion",
    "href": "posts/startup/startup.html#conclusion",
    "title": "Startup script for my Linux desktop",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, I’ve shared my startup.sh script for my Linux desktop, which opens various apps and webpages when I log in and arranges them on my workspaces. I’ve also explained how I test and debug the script, and shown how I am developing it as a Jupyter notebook with the Bash kernel. By exporting the shell script from the notebook, I can easily run it whenever I need to. I hope that this script is useful as an example to others who want to automate their desktop environment and increase their productivity; or at least a little interesting.\nI like this script in that helps me keep my workspaces organized right away when I log in. However, I spend too much time tweaking it, which can eat up valuable time that could be spent on actual work.\nTo be honest, I think it was fairly bonkers of me to go to this level of effort to set up my windows just so at startup. You probably shouldn’t imitate such bizarre nerdish behaviour!\nThanks for reading."
  },
  {
    "objectID": "posts/startup/startup.html#appendix-1-tools-and-programs-that-i-use-here",
    "href": "posts/startup/startup.html#appendix-1-tools-and-programs-that-i-use-here",
    "title": "Startup script for my Linux desktop",
    "section": "Appendix 1: tools and programs that I use here",
    "text": "Appendix 1: tools and programs that I use here\n\nJupyter notebook with the Bash kernel for developing and testing my startup script\nBash shell scripting language\nGnome desktop environment\nXorg display server\nsetxkbmap allows to change the keyboard layout and options\nwmctrl for controlling windows on your desktop\nxdotool for simulating keyboard and mouse input, and controlling windows\nChrome web browser\nTrello for task planning\nChatGPT for an AI assistant\nMiniflux for a blog reader\nAnki for learning with flashcards\nPerl programming language for the xtile script\nScreen for creating and managing terminal sessions\nSlack for team communication\nDiscord for voice and text chat\nNautilus file manager\nZotero for reference management\nGnome Pomodoro for timing my work and taking breaks\nxterm for a command line with a nice bitmap font (misc fixed 6x13)\nssh for connecting to remote servers using the command-line interface.\nBarrier for sharing a keyboard and mouse across multiple computers as a software KVM switch."
  },
  {
    "objectID": "posts/startup/startup.html#appendix-2-scripts-that-i-wrote-and-used-here",
    "href": "posts/startup/startup.html#appendix-2-scripts-that-i-wrote-and-used-here",
    "title": "Startup script for my Linux desktop",
    "section": "Appendix 2: scripts that I wrote and used here",
    "text": "Appendix 2: scripts that I wrote and used here\n\ngnome-dark: A Bash script that sets the Gnome desktop environment to use the Adwaita-dark theme and a dark color scheme.\ngnome-light: A Bash script that sets the Gnome desktop environment to use the Adwaita theme and a light color scheme.\nconnects: A Bash script that connects to several remote hosts using the connect command and runs them in the background.\nconnect: A Bash script that uses ssh to connect to remote servers specified as command-line arguments, with some connection options set.\nchrome_profile: A Bash script that launches Google Chrome with a specific profile.\nchrome_profile_key: A Bash script that returns the key for a specific Chrome profile.\nchrome_profiles_list: A Bash script that lists all Chrome profiles and their keys.\nswatch: A Bash script that launches a terminal window and displays system information using the watch command and other tools.\nxtile: A Perl script that tiles a window on a specific screen, with options for the window size and position.\ns: A Bash script that launches the screen command with some default options, and any additional arguments.\nsw: A Bash script that creates a new screen and window, with an optional directory.\nsxw: A Bash script that attaches to a specific screen window using the screen command.\nsstuff: A Bash script that sends keystrokes to a specific screen window using the screen command."
  },
  {
    "objectID": "posts/blog/blog.html",
    "href": "posts/blog/blog.html",
    "title": "Blogging AI with Jupyter and Quarto",
    "section": "",
    "text": "Welcome to my blog! My name is Sam Watkins, and I am a programmer and math enthusiast with a keen interest in artificial intelligence. As part of my journey to learn more about AI, I am starting this blog to document my studies, particularly with the fast.ai book and courses.\nIn this first post, I will provide an overview of why I am starting this blog and share some details on how I plan to use Jupyter, Quarto, and nbdev2 tools to create software libraries, documentation, apps, and blog posts. I believe that blogging will not only help me solidify my understanding of AI but also provide a platform to share my insights and get feedback from others in the community. Blogging can be a powerful tool for learning, networking, and personal development.\nI want to note from the beginning, that I am using ChatGPT to help me write better blog posts; but I’m doing most of the work!"
  },
  {
    "objectID": "posts/blog/blog.html#why-write-a-blog",
    "href": "posts/blog/blog.html#why-write-a-blog",
    "title": "Blogging AI with Jupyter and Quarto",
    "section": "Why write a blog?",
    "text": "Why write a blog?\nIn a previous post on dev.to, I discussed the benefits of keeping a development log. Writing about my progress helps me learn more effectively, and the feedback I receive can be invaluable. I’ve also learned from the success of other developers, like the creator of the game Gunpoint, who grew a community around his blog and received feedback at every step of his project.\nI hope that by starting this blog, I can experience some of the benefits of blogging for myself and maybe help others in the process. Please leave a comment using the hypothes.is plugin on the right side, if you have any thoughts on the topic or suggestions for me as I begin this journey.\n\n\nSee also\n\n\nThis previous post of mine on dev.to: Why Write a Dev Log?\nRachael Thomas’s post: Why you (yes, you) should blog\nAnother post from Rachael: Advice for Better Blog Posts\nAn exemplary dev log: Gunpoint dev log\nAn exemplary dev retrospective: Gunpoint dev breakdown"
  },
  {
    "objectID": "posts/blog/blog.html#why-use-jupyter-and-quarto",
    "href": "posts/blog/blog.html#why-use-jupyter-and-quarto",
    "title": "Blogging AI with Jupyter and Quarto",
    "section": "Why use Jupyter and Quarto?",
    "text": "Why use Jupyter and Quarto?\nThe fast.ai team recommends to work in Jupyter and blog using Quarto. Their nbdev2 tool also makes use of Quarto. We can work on a notebooks in Jupyter, then run them through nbdev2 and Quarto to produce software libraries, documentation, apps, blog posts, papers, and even books.\n\n\nSee also\n\n\nfast.ai: A company that provides practical deep learning courses and libraries, as well as open source software tools for building and deploying deep learning models.\nnbdev+Quarto: A new secret weapon for productivity: The blog post recommending to use Jupyter with nbdev and Quarto.\nJupyter: A web-based interactive development environment for creating notebooks, code, and data visualizations. Supports multiple programming languages.\nQuarto: A document automation tool that can be used for creating books, reports, and blogs using Markdown, Jupyter Notebooks, and other file formats.\nnbdev: A tool that helps you develop Python packages using Jupyter Notebooks."
  },
  {
    "objectID": "posts/blog/blog.html#installing-the-software",
    "href": "posts/blog/blog.html#installing-the-software",
    "title": "Blogging AI with Jupyter and Quarto",
    "section": "Installing the software",
    "text": "Installing the software\nI am running the AI tools on my home PC, with Debian GNU/Linux and an NVIDIA GPU. I’ll write more about my setup in another post.\nI downloaded Quarto from their Get Started page, and installed it.\nTo manage the installation of Python packages, I’m using pip, which is a package installer for Python. It’s a command-line utility that helps you install and manage software packages written in Python. You can learn more about pip and its usage from the official pip documentation.\nThis command installs or upgrades Jupyter, fastbook, and nbdev Python packages, which I’ll need to use for AI experiments and to help me write my blog.\n\npip install -qq -U jupyter fastbook nbdev"
  },
  {
    "objectID": "posts/blog/blog.html#setting-up-the-blog",
    "href": "posts/blog/blog.html#setting-up-the-blog",
    "title": "Blogging AI with Jupyter and Quarto",
    "section": "Setting up the blog",
    "text": "Setting up the blog\nTo set up the blog, I first created a Git repository called ai in my home directory. I used the following commands to create the blog project.\n\ncd\nmkdir -p ai\ncd ai\ngit init\n\nReinitialized existing Git repository in /home/sam/ai/.git/\n\n\n\nquarto create-project blog --type website:blog\n\nCreating project at /home/sam/ai/blog:\n  - Created _quarto.yml\n  - Created .gitignore\n  - Created index.qmd\n  - Created posts/welcome/index.qmd\n  - Created posts/post-with-code/index.qmd\n  - Created about.qmd\n  - Created styles.css\n  - Created posts/_metadata.yml\n\n\nI also ran the command nbdev_install_hooks to prevent unnecessary Git merge conflicts and enable conflict resolution in Jupyter.\n\nnbdev_install_hooks\n\nHooks are installed.\n\n\n\n\nSee also\n\n\nQuarto - Creating a Blog\nThe Jupyter+git problem is now solved\nGit-Friendly Jupyter\nGit documentation\nGit Handbook\nCodecademy’s “Learn Git” course\nBash Guide for Beginners"
  },
  {
    "objectID": "posts/blog/blog.html#adding-this-post-to-the-blog",
    "href": "posts/blog/blog.html#adding-this-post-to-the-blog",
    "title": "Blogging AI with Jupyter and Quarto",
    "section": "Adding this post to the blog",
    "text": "Adding this post to the blog\nI had already started writing this post in a Jupyter Lab notebook blogging.ipynb. Now I needed to put it in the right place, so I made a directory for it:\nI started writing this post in a Jupyter Lab notebook called blogging.ipynb. When I got to this point, I needed to put it in the right directory to include it in the blog. I first created a directory called blog/posts/blogging:\n\nmkdir -p blog/posts/blogging\n\nThen I simply moved the blogging.ipynb file to that directory using Jupyter Lab’s file browser. This allowed me to easily include the post in the blog without even having to close the notebook."
  },
  {
    "objectID": "posts/blog/blog.html#previewing-the-blog",
    "href": "posts/blog/blog.html#previewing-the-blog",
    "title": "Blogging AI with Jupyter and Quarto",
    "section": "Previewing the blog",
    "text": "Previewing the blog\nOnce you’ve set up your blog, you can preview it by running the following command in a terminal:\nquarto preview blog\nMake sure to run this command outside of Jupyter, as it can cause Jupyter to hang if run in a cell.\nYou can also use additional options to customize the preview. For example, if you don’t want to launch a browser, you can use the –no-browser option. If you want to specify a custom port, you can use the –port option followed by the desired port number. Here’s an example of how to use both options:\nquarto preview blog --no-browser --port 4242\nThis will start the preview on port 4242 and prevent it from automatically launching a browser. I do it this way myself, because I open the preview window from a separate startup script."
  },
  {
    "objectID": "posts/blog/blog.html#finishing-up",
    "href": "posts/blog/blog.html#finishing-up",
    "title": "Blogging AI with Jupyter and Quarto",
    "section": "Finishing up",
    "text": "Finishing up\nTo clean up the blog ready for publishing my first post, I removed the example posts, changed the settings, filled in the about page, added links to my social media profiles, and added a profile picture.\nI also followed the Quarto documentation to enable comments using Hypothesis and an RSS feed for the blog.\nThe Hypothesis annotation tool to allow readers to leave comments and annotations on the blog posts. This feature required me to create an Hypothesis account and link it to the blog. It’s functional, but looks and feels different from regular commenting. Please try it out and leave a comment for me.\nFor the RSS feed, I followed the Quarto guide to add an RSS feed to the blog. This enables readers to subscribe to the blog and receive updates whenever new posts are published. Personally I am using the Miniflux RSS reader; Newsblur is another good option. After a bit of research, I found that I can follow the comments on my blog using this RSS stream.\n\n\nSee also\n\n\nQuarto - HTML Basics - Commenting\nQuarto - Creating a Blog - RSS Feed\nHypothesis: A web-based annotation tool that allows users to highlight and comment on web pages.\nRSS: A web feed format used to publish frequently updated content such as blog entries, news headlines, and podcasts.\nMiniflux: A minimalist and open source RSS reader.\nNewsBlur: A personal news reader that brings people together to talk about the world."
  },
  {
    "objectID": "posts/blog/blog.html#committing-to-git",
    "href": "posts/blog/blog.html#committing-to-git",
    "title": "Blogging AI with Jupyter and Quarto",
    "section": "Committing to git",
    "text": "Committing to git\nThe Quarto doc says “you should always fully quarto render your site before deploying it”, so let’s do it, and commit to git:\n\ncd ~/ai\nquarto render blog\ngit add blog\ngit commit -m 'update blog'\ngit push\ngit push opal\ngit push pi\n\nI pushed to github, and my two servers opal and pi.\n\n\nSee also\n\n\nQuarto - Websites\nGitHub"
  },
  {
    "objectID": "posts/blog/blog.html#publishing-the-blog",
    "href": "posts/blog/blog.html#publishing-the-blog",
    "title": "Blogging AI with Jupyter and Quarto",
    "section": "Publishing the blog",
    "text": "Publishing the blog\nI used a symlink on my server to put the rendered blog into my website at sam.ucm.dev/blog:\nln -s ~/ai/blog/_site ~/www/sam.ucm.dev/blog\n\n\nSee also\n\n\nQuarto - Publishing"
  },
  {
    "objectID": "posts/blog/blog.html#conclusion",
    "href": "posts/blog/blog.html#conclusion",
    "title": "Blogging AI with Jupyter and Quarto",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we’ve seen how to set up a blog using Quarto and Jupyter. These tools allow us to write and publish blog posts as Jupyter notebooks, which can be converted to various formats such as HTML, PDF, and LaTeX, among others. We’ve also looked at using Hypothesis for comments and annotations, and how to commit our changes to git and deploy our blog on a personal server. By following these steps, we can create a blog that showcases our AI studies and provides a platform for getting feedback, sharing knowledge, and building a community.\nOne thing I would like to explore further, is a way that I can automatically link to other AI bloggers in the community. Link sharing and discovery is one of the benefits of posting on an off-the-shelf playform, such as Towards Data Science; it would be good to have something similar for indie bloggers. Maybe just an RSS aggregator plugin that would show links to other recent posts from selected friends and communities…\nIf you have any questions or suggestions, please leave a comment, if you can figure out how to use Hypothesis. Thanks for reading!"
  },
  {
    "objectID": "posts/setup/setup.html",
    "href": "posts/setup/setup.html",
    "title": "My setup for AI work on my Debian PC",
    "section": "",
    "text": "This post on the Fastai forum is helpful: For those who run their own AI box, or want to.\nIf you use Ubuntu, Lambda Stack looks good."
  },
  {
    "objectID": "posts/setup/setup.html#introduction",
    "href": "posts/setup/setup.html#introduction",
    "title": "My setup for AI work on my Debian PC",
    "section": "Introduction",
    "text": "Introduction\nThis blog post is highly technical, so with the help of ChatGPT I have included a glossary of terms and concepts for each section. Thanks ChatGPT, I wouldn’t have done that without you!\nI am using my home PC for AI work, with Debian GNU/Linux 12 “bookworm” (testing) and an NVIDIA GPU.\nThe main AI tools that I use are:\n\nJupyter for experiments, development and blogging;\nfastai for training neural networks;\nHuggingface tools such as transformers and diffusers;\nPyTorch, Tensorflow and other lower-level AI libraries; and\nstable-diffusion-webui for experimenting with AI image generation.\n\nThis document is mostly for my own reference. It wasn’t so easy getting everything working nicely and I want to have a record of how I did it. I don’t recommend that you try to do this unless you are experienced with Debian, and you don’t mind taking time to deal with problems when they occur.\nI wanted to avoid using docker, conda, or Python venvs. Instead, I installed the necessary Python modules under /usr/local with pip. I was thinking that I would be able to use my AI tools directly from the command line and other scripts, like any other tools.\nThis worked for a little while, but then Debian started using Python 3.11 for their default Python. Torch isn’t compatible with Python 3.11 yet, and everything broke for me.\nAfter trying to fix it for a while and encountering numerous problems, I decided to switch to using Python 3.10. After more problems, I switched to using Python 3.10 venvs. I think that the original method of installing under /usr/local could work, but the Debian python packages are in a mess state of flux at the moment.\nAs they say, using a virtual environment is a best practice in Python development and can help avoid issues with package conflicts and dependency management. So, venvs it is!\n\n\nGlossary\n\n\nconda: a package management system and environment management system for installing multiple versions of software packages and their dependencies, including both Python and non-Python packages.\nDebian: a free and open-source operating system based on the Linux kernel, widely used for servers and workstations\nDebian “testing”: a rolling release version of Debian that is in the process of being tested for the next stable release, and contains newer packages than the current stable release. It is not recommended for production use.\ndependency management: the process of identifying and resolving dependencies between software packages, to ensure that they can be installed and run together without conflicts\ndocker: a platform for developing, shipping, and running applications using containers, which are lightweight, portable, and self-contained environments that run applications and their dependencies.\nFastai: a free, open-source deep learning library built on top of PyTorch that provides a high-level API and a range of state-of-the-art models and techniques\nGNU: A project started by Richard Stallman in 1983 to create a free and open-source operating system, consisting of a complete set of tools and utilities to replace proprietary software. GNU stands for “GNU’s Not Unix”.\nGNU/Linux: A term used to describe the operating system that consists of the GNU tools and the Linux kernel. The GNU tools provide the user interface and the software development tools, while the Linux kernel provides the low-level system functions, such as process management, memory management, and device drivers.\nHugging Face 🤗: a company that develops tools for building machine learning apps, with a focus on natural language processing (NLP).\nHugging Face Diffusers: a popular open-source library that provides pretrained vision and audio diffusion models, and serves as a modular toolbox for inference and training\nHugging Face Hub: a platform that allows users to share machine learning models and datasets.\nJupyter: an open-source web application that allows users to create and share documents that contain live code, equations, visualizations and narrative text\nLinux: A kernel, or the core component of an operating system, originally created by Linus Torvalds in 1991. Linux is based on Unix, and is released under an open-source license, which allows anyone to modify and distribute the source code. The combination of the Linux kernel and the GNU tools forms the GNU/Linux operating system.\nNeural networks: a machine learning technique that allows computers to learn from data by adjusting the strengths of connections between neurons\nNVIDIA GPU: a graphics processing unit manufactured by NVIDIA that is commonly used for machine learning and other compute-intensive tasks\npackage conflicts: situations where two or more Python packages require different versions of the same dependency, leading to issues when trying to install or run the packages together\npip: a package installer for Python that allows you to easily install and manage third-party packages and their dependencies. It is commonly used with virtual environments to manage Python package dependencies.\nPyTorch: a popular open-source machine learning library based on the Torch library\nStable-diffusion-webui: an open-source web user interface for state-of-the-art image generation b based on stable diffusion, by automatic1111\nTensorflow: an open-source machine learning library developed by Google Brain Team\ntransformers: a state-of-the-art library for natural language processing (NLP) tasks such as text classification and question answering, built by Huggingface\n/usr/local: a directory in Unix-like operating systems that is typically used for installing software manually, outside of the system package manager.\nvirtual environment (venv): a self-contained Python environment that allows you to install and manage packages without affecting the system-level Python installation or other virtual environments"
  },
  {
    "objectID": "posts/setup/setup.html#debian-apt-setup",
    "href": "posts/setup/setup.html#debian-apt-setup",
    "title": "My setup for AI work on my Debian PC",
    "section": "Debian apt setup",
    "text": "Debian apt setup\nI installed some NVIDIA packages and alternative Python versions from repositories which are intended for Ubuntu, which is a bit hacky, but it can work. The following shows how to create a FrankenDebian install using some packages that were intended for Ubuntu, without totally trashing your system.\n\n\nGlossary\n\n\nFrankenDebian: a term used to describe a Debian installation that has been modified or customized in non-standard ways, often resulting in instability or other issues\nNVIDIA packages: software components and drivers provided by NVIDIA Corporation to support their graphics processing units (GPUs)\nPython versions: different releases of the Python programming language, each with its own set of features and bug fixes\nrepositories: online locations where software packages can be downloaded and installed from, typically maintained by a software distributor or community\nUbuntu: a popular distribution of the GNU/Linux operating system, known for its ease of use and large user community\n\n\n\nDon’t break Debian: release pinning\nRelease pinning in Debian is a way of specifying which versions of packages to install from which Debian releases. The code snippet provided shows a file I added called 99dontbreakdebian in the /etc/apt/preferences.d/ directory, with pins for various package releases. These pins specify a release or origin and a priority, which determines which package version to install if multiple versions are available. By setting these pins, I can ensure that my system installs packages from the desired release and avoid accidentally breaking the system by installing incompatible package versions, while still being able to install packages from other sources as needed.\n\ncat /etc/apt/preferences.d/99dontbreakdebian\n\nPackage: *\nPin: release o=Debian,a=experimental\nPin-Priority: 1\n\nPackage: *\nPin: release o=Debian,a=unstable\nPin-Priority: 90\n\nPackage: *\nPin: release a=focal\nPin-Priority: 70\n\nPackage: *\nPin: release a=jammy\nPin-Priority: 80\n\nPackage: *\nPin: release o=LP-PPA-deadsnakes\nPin-Priority: 90\n\nPackage: *\nPin: origin ppa.launchpad.net\nPin-Priority: 90\n\nPackage: *\nPin: release o=Debian\nPin-Priority: 990\n\n\n\n\nGlossary\n\n\nRelease pinning: A way of specifying which versions of packages to install from which Debian releases.\n/etc/apt/preferences.d/: A directory in Debian where you can add files to specify package release pins.\nPin: A rule that specifies a release or origin and a priority for a package.\nExperimental: The Debian release channel where packages are the least stable, but often contain the latest features and updates.\nUnstable: The Debian release channel where packages are more stable than experimental, but still not considered release quality.\nFocal and Jammy: Ubuntu release code names.\nLP-PPA-deadsnakes: A Personal Package Archive (PPA) on Launchpad for providing alternative versions of Python for Ubuntu and Debian systems.\n\n\n\n\nMy main apt sources.list\n\ncat /etc/apt/sources.list\n\ndeb http://deb.debian.org/debian/ bookworm main contrib non-free\ndeb http://deb.debian.org/debian/ sid main contrib non-free\ndeb-src http://deb.debian.org/debian/ bookworm main contrib non-free\ndeb-src http://deb.debian.org/debian/ sid main contrib non-free\n\ndeb http://security.debian.org/debian-security bookworm-security main contrib non-free\ndeb-src http://security.debian.org/debian-security bookworm-security main contrib non-free\n\ndeb http://deb.debian.org/debian/ bookworm-updates main contrib non-free\ndeb-src http://deb.debian.org/debian/ bookworm-updates main contrib non-free\n\ndeb http://deb.debian.org/debian/ bookworm-backports main contrib non-free\ndeb-src http://deb.debian.org/debian/ bookworm-backports main contrib non-free\n\ndeb http://deb.debian.org/debian/ experimental main contrib non-free\ndeb-src http://deb.debian.org/debian/ experimental main contrib non-free\n\n\n\n\nGlossary\n\n\nsources.list: a configuration file in Debian-based systems that specifies the package repositories from which the system can download and install software.\ndeb: a Debian binary package format used to distribute software packages for Debian and its derivatives, including Ubuntu and Mint.\nbookworm: This is the codename for the Debian 12 release, currently in testing, which will become the next stable release.\nsid: the codename for Debian’s unstable release.\nmain: This is one of the Debian software repositories, which contains the core packages that make up the Debian operating system.\ncontrib and non-free: two categories of software packages in Debian that include packages with dependencies on non-free or proprietary software. deb-src: This stands for “debian source”. It refers to the software source code repositories for Debian, which users can access in order to download, modify, and compile the source code of various packages.\nsecurity: This is the Debian repository that contains security updates for Debian packages. It is important to include this repository in your sources.list file to ensure that your system stays secure.\nbackports: This is the Debian repository that contains newer versions of packages that have been backported from newer Debian releases.\nexperimental repo: This is the Debian repository that contains packages that are still in the testing phase and are not yet stable enough for inclusion in the main Debian repositories.\n\n\n\n\nApt sources for NVIDIA CUDA, cuDNN, TensorRT, and containers\nWe will use some packages that were built for Ubuntu, but it doesn’t seem to cause a problem.\n\ncd /etc/apt/sources.list.d\n\n\ncat cuda-debian11-x86_64.list\n\ndeb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos/debian11/x86_64/ /\n\n\n\ncat cudnn-local-debian11-8.6.0.163.list \n\ndeb [signed-by=/usr/share/keyrings/cudnn-local-C922C4FD-keyring.gpg] file:///var/cudnn-local-repo-debian11-8.6.0.163 /\n\n\n\ncat nv-tensorrt-local-ubuntu2204-8.5.3-cuda-11.8.list\n\ndeb [signed-by=/usr/share/keyrings/nv-tensorrt-local-3E951519-keyring.gpg] file:///var/nv-tensorrt-local-repo-ubuntu2204-8.5.3-cuda-11.8 /\n\n\n\ncat nvidia-container-runtime.list\n\ndeb https://nvidia.github.io/libnvidia-container/stable/debian11/$(ARCH) /\n# deb https://nvidia.github.io/libnvidia-container/experimental/debian10/$(ARCH) /\ndeb https://nvidia.github.io/nvidia-container-runtime/stable/debian11/$(ARCH) /\n# deb https://nvidia.github.io/nvidia-container-runtime/experimental/debian10/$(ARCH) /\n\n\n\n\nGlossary\n\n\nNVIDIA CUDA: a parallel computing platform and programming model for NVIDIA GPUs\ncuDNN: a GPU-accelerated library for deep neural networks\nTensorRT: a high-performance deep learning inference optimizer and runtime library\nContainers: lightweight, standalone executables that include everything needed to run a piece of software, including the code, libraries, and system tools\nNVIDIA container runtime: a platform developed by NVIDIA to support containerized applications that require access to GPU resources.\n\n\n\n\nApt sources for old versions of Python\nThe “deadsnakes” PPA was built for Ubuntu, but works fine on Debian too.\n\ncat deadsnakes.list\n\ndeb http://ppa.launchpad.net/deadsnakes/ppa/ubuntu jammy main\ndeb-src http://ppa.launchpad.net/deadsnakes/ppa/ubuntu jammy main\n\n\n\n\nGlossary\n\n\ndeadsnakes: A Personal Package Archive (PPA) on Launchpad for providing alternative versions of Python for Ubuntu and Debian systems.\nPPA: Stands for Personal Package Archive, a software repository for Ubuntu users that allows them to distribute software and updates that are not available in official Ubuntu repositories.\njammy: The code name for Ubuntu 22.04.2 LTS (Jammy Jellyfish), the version of Ubuntu that the deadsnakes PPA was built for."
  },
  {
    "objectID": "posts/setup/setup.html#install-required-debian-packages",
    "href": "posts/setup/setup.html#install-required-debian-packages",
    "title": "My setup for AI work on my Debian PC",
    "section": "Install required Debian packages",
    "text": "Install required Debian packages\n\nPython 3.10\nAs I mentioned, Debian recently started using Python 3.11 for their default Python, and Torch isn’t compatible with Python 3.11 yet:\n\npython3.11 -m pip install --break-system-packages torchvision\n\nERROR: Could not find a version that satisfies the requirement torchvision (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3)\nERROR: No matching distribution found for torchvision\n\n\n\n: 1\n\n\nI tried using nightly Torch, but there were more compatibility problems. So I decided to go back to using Python 3.10 for AI work:\n\nsudo apt install -qq python3.10-venv\n\nSome packages could not be installed. This may mean that you have\nrequested an impossible situation or if you are using the unstable\ndistribution that some required packages have not yet been created\nor been moved out of Incoming.\nThe following information may help to resolve the situation:\n\nThe following packages have unmet dependencies:\n python3.10-venv : Depends: python3.10-distutils but it is not installable\nE: Unable to correct problems, you have held broken packages.\n\n\n: 100\n\n\nUnfortunately, the packaging for python3.10-venv is currently broken. I solved this by making a python3.10-distutils-bogus package, like this:\n\ncd ~/pkg\ncat python3.10-distutils-bogus\n\nSection: python\nPriority: optional\nStandards-Version: 3.9.2\n\nPackage: python3.10-distutils-bogus\nVersion: 1.0\nMaintainer: Sam Watkins <sam@ucm.dev>\nProvides: python3.10-distutils\nDescription: Dummy package to satisfy python3.10-distutils\n\n\n\nequivs-build python3.10-distutils-bogus\n\ndpkg-buildpackage: info: source package python3.10-distutils-bogus\ndpkg-buildpackage: info: source version 1.0\ndpkg-buildpackage: info: source distribution unstable\ndpkg-buildpackage: info: source changed by Sam Watkins <sam@ucm.dev>\ndpkg-buildpackage: info: host architecture amd64\n dpkg-source --before-build .\n debian/rules clean\ndh clean\n   dh_clean\n debian/rules binary\ndh binary\n   dh_update_autotools_config\n   dh_autoreconf\n   create-stamp debian/debhelper-build-stamp\n   dh_prep\n   dh_auto_install --destdir=debian/python3.10-distutils-bogus/\n   dh_install\n   dh_installdocs\n   dh_installchangelogs\n   dh_perl\n   dh_link\n   dh_strip_nondeterminism\n   dh_compress\n   dh_fixperms\n   dh_missing\n   dh_installdeb\n   dh_gencontrol\n   dh_md5sums\n   dh_builddeb\ndpkg-deb: building package 'python3.10-distutils-bogus' in '../python3.10-distutils-bogus_1.0_all.deb'.\n dpkg-genbuildinfo --build=binary -O../python3.10-distutils-bogus_1.0_amd64.buildinfo\n dpkg-genchanges --build=binary -O../python3.10-distutils-bogus_1.0_amd64.changes\ndpkg-genchanges: info: binary-only upload (no source code included)\n dpkg-source --after-build .\ndpkg-buildpackage: info: binary-only upload (no source included)\n\nThe package has been created.\nAttention, the package has been created in the current directory,\nnot in \"..\" as indicated by the message above!\n\n\n\nsudo dpkg -i ./python3.10-distutils-bogus_1.0_all.deb\n\n(Reading database ... 1158515 files and directories currently installed.)\nPreparing to unpack .../python3.10-distutils-bogus_1.0_all.deb ...\nUnpacking python3.10-distutils-bogus (1.0) over (1.0) ...\nSetting up python3.10-distutils-bogus (1.0) ...\n\n\n\nsudo apt-get -q install python3.10-venv\n\nReading package lists...\nBuilding dependency tree...\nReading state information...\npython3.10-venv is already the newest version (3.10.10-2).\n0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n\n\n\n\nGlossary\n\n\nDebian package dependencies: packages that are required for a specific package to install and function properly\npython3.11: the default version of Python now used in “bookworm” the latest Debian testing release, and “sid” (unstable))\npython3.10-venv: A package that provides the “venv” module for Python 3.10, which is used to create Python virtual environments.\npython3.10-distutils: a module in Python 3.10 that provides tools for building and installing Python modules and packages\nequivs-build: a tool for creating Debian packages that provide empty or dummy packages to satisfy dependencies\ndpkg-buildpackage: a tool for building Debian packages from source code\nbroken packages: Packages that cannot be installed due to missing dependencies or conflicts with other packages.\nTorch: A popular machine learning library, which is not compatible with Python 3.11 yet.\nPython 3.10: A version of Python that is compatible with Torch.\nMaintainer: The person or organization responsible for maintaining a package in the Debian package repository.\nProvides: A field in a Debian package control file that specifies the name of a package that the current package provides. This can be used to satisfy dependencies of other packages that require the provided package.\n\n\n\n\nOld and alpha versions of Python, from deadsnakes\nI’m not actually using these alternative Python packages for AI at the moment.\nHowever, it’s often useful to be able to use different versions of Python, so it’s an important part of my setup.\n\nPython 3.11 is the new default version for Debian.\nPython 3.10 is still available in Debian, it’s the one we need to use.\nI have Python 2.7 left over from a previous Debian release.\nI installed 3.7, 3.8, 3.9 and 3.12 from deadsnakes.\n\n\nsudo apt install -qq -t jammy python3.7-venv python3.8-venv python3.9-venv\n\npython3.7-venv is already the newest version (3.7.16-1+jammy1).\npython3.8-venv is already the newest version (3.8.16-1+jammy1).\npython3.9-venv is already the newest version (3.9.16-1+jammy1).\n0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n\n\n\nsudo apt install -qq -t jammy python3.12-venv\n\npython3.12-venv is already the newest version (3.12.0~a5-1+jammy2).\n0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n\n\n\napt policy python2; echo\nfor v in `seq 7 12`; do apt policy python3.$v; echo; done\n\npython2:\n  Installed: 2.7.18-3\n  Candidate: 2.7.18-3\n  Version table:\n *** 2.7.18-3 100\n        100 /var/lib/dpkg/status\n\npython3.7:\n  Installed: 3.7.16-1+jammy1\n  Candidate: 3.7.16-1+jammy1\n  Version table:\n *** 3.7.16-1+jammy1 100\n         80 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages\n        100 /var/lib/dpkg/status\n\npython3.8:\n  Installed: 3.8.16-1+jammy1\n  Candidate: 3.8.16-1+jammy1\n  Version table:\n *** 3.8.16-1+jammy1 100\n         80 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages\n        100 /var/lib/dpkg/status\n\npython3.9:\n  Installed: 3.9.16-1+jammy1\n  Candidate: 3.9.16-1+jammy1\n  Version table:\n *** 3.9.16-1+jammy1 100\n         80 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages\n        100 /var/lib/dpkg/status\n\npython3.10:\n  Installed: 3.10.10-2\n  Candidate: 3.10.10-2\n  Version table:\n *** 3.10.10-2 100\n         90 http://deb.debian.org/debian sid/main amd64 Packages\n        100 /var/lib/dpkg/status\n\npython3.11:\n  Installed: 3.11.2-4\n  Candidate: 3.11.2-4\n  Version table:\n *** 3.11.2-4 990\n        990 http://deb.debian.org/debian bookworm/main amd64 Packages\n         90 http://deb.debian.org/debian sid/main amd64 Packages\n        100 /var/lib/dpkg/status\n     3.11.2-1+jammy1 80\n         80 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages\n\npython3.12:\n  Installed: 3.12.0~a5-1+jammy2\n  Candidate: 3.12.0~a5-1+jammy2\n  Version table:\n *** 3.12.0~a5-1+jammy2 100\n         80 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages\n        100 /var/lib/dpkg/status\n\n\n\n\n\nGlossary\n\n\ndeadsnakes PPA: A Personal Package Archive (PPA) containing various versions of Python packages, maintained by the deadsnakes team on Launchpad.\nPython 3.11: The new default version of Python for Debian, which is not yet compatible with Torch.\nPython 3.10: A compatible version of Python that is still available in Debian and needed to use Torch.\nPython 2.7: A legacy version of Python that I had installed from a previous Debian release.\nPython 3.7, 3.8, 3.9, 3.12: Alternative versions of Python that I installed from the deadsnakes PPA; not strictly needed for most AI work.\n\n\n\n\nNVIDIA CUDA\n\nsudo apt -qq install cuda=12.0.1-1 cuda-drivers=525.85.12-1 cuda-11-7 cuda-11-8\n\ncuda is already the newest version (12.0.1-1).\ncuda-drivers is already the newest version (525.85.12-1).\ncuda-11-7 is already the newest version (11.7.1-1).\ncuda-11-8 is already the newest version (11.8.0-1).\n0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n\n\n\n\nGlossary\n\n\nNVIDIA CUDA: A parallel computing platform and programming model developed by NVIDIA for general computing on GPUs (graphics processing units).\nCUDA drivers: Software components that enable communication between the CUDA runtime and the hardware.\nCUDA 11-7, CUDA 11-8 and CUDA 12-0: Different versions of the CUDA toolkit that are compatible with different GPU architectures.\n\n\n\n\nNVIDIA Container Runtime\n\nsudo apt -qq install nvidia-container-runtime\n\nnvidia-container-runtime is already the newest version (3.12.0-1).\n0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n\n\n\n\nGlossary\n\n\nNVIDIA Container Runtime: An open-source container runtime that enables the use of GPUs within containers, allowing applications to leverage the power of NVIDIA GPUs while maintaining the flexibility and portability of containerization. It is designed to work with Docker and other container engines and is optimized for use with NVIDIA GPUs.\nDocker: A popular platform for developing, deploying, and running applications in containers. It provides a way to package an application and its dependencies into a single container that can be easily moved between environments.\nGPU: Short for Graphics Processing Unit, a specialized processor designed to accelerate the rendering of images and video. In recent years, GPUs have become increasingly popular for running compute-intensive workloads, such as machine learning and scientific simulations.\n\n\n\n\nNVIDIA cuDNN\n\nsudo apt -qq install libcudnn8-dev\n\nlibcudnn8-dev is already the newest version (8.8.0.121-1+cuda12.0).\n0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n\n\n\nsudo apt -qq install /var/cudnn-local-repo-ubuntu2204-8.6.0.163/libcudnn8-samples_8.6.0.163-1+cuda11.8_amd64.deb\n\nlibcudnn8-samples is already the newest version (8.6.0.163-1+cuda11.8).\n0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n\n\n\n\nGlossary\n\n\nNVIDIA cuDNN: A GPU-accelerated library for deep neural networks that is used to improve training and inference performance.\nlibcudnn8-dev: A package that provides the development files needed to compile software that uses NVIDIA cuDNN.\nlibcudnn8-samples: A package that contains sample code and programs that demonstrate how to use NVIDIA cuDNN in applications.\n\n\n\n\nNVIDIA TensorRT\nNVIDIA TensorRT is an inference accelerator for deep learning models. It optimizes and deploys trained neural networks for inferencing on NVIDIA GPUs. In order to use TensorRT, the python3-libnvinfer package needs to be installed.\nUnfortunately, this package has a dependency issue, it requires a version of Python less than 3.11 for the package to work, but I couldn’t downgrade the system’s Python version. To resolve this issue, I modified the package to depend on the python3.10 package instead of python3 < 3.11. I did this by copying the package to a local directory, unpacking it, modifying the dependency information, and repacking it. After this, I was able to successfully install the modified package and its development version using the dpkg command.\nFinally, I was able to install the main tensorrt package.\n\nsudo apt-get -q install python3-libnvinfer-dev\n\nReading package lists...\nBuilding dependency tree...\nReading state information...\nSome packages could not be installed. This may mean that you have\nrequested an impossible situation or if you are using the unstable\ndistribution that some required packages have not yet been created\nor been moved out of Incoming.\nThe following information may help to resolve the situation:\n\nThe following packages have unmet dependencies:\n python3-libnvinfer : Depends: python3 (< 3.11) but 3.11.2-1 is to be installed\nE: Unable to correct problems, you have held broken packages.\n\n\n: 100\n\n\nI’d like to install it as a python3.10 library at least, but it is demanding that the system Python version, i.e. the version of Debian’s python3 package, should be less than 3.11, and I don’t want to try to change that. So I’ll adjust this python3-libnvinfer package to depend on the python3.10 package instead of python3 < 3.11.\n\ncd /var/nv-tensorrt-local-repo-ubuntu2204-8.5.3-cuda-11.8\nls python3-libnvinfer*\n\npython3-libnvinfer_8.5.3-1+cuda11.8_amd64.deb\npython3-libnvinfer-dev_8.5.3-1+cuda11.8_amd64.deb\n\n\n\ncp -v python3-libnvinfer* ~/soft-ai/\n\n'python3-libnvinfer_8.5.3-1+cuda11.8_amd64.deb' -> '/home/sam/soft-ai/python3-libnvinfer_8.5.3-1+cuda11.8_amd64.deb'\n'python3-libnvinfer-dev_8.5.3-1+cuda11.8_amd64.deb' -> '/home/sam/soft-ai/python3-libnvinfer-dev_8.5.3-1+cuda11.8_amd64.deb'\n\n\n\ncd ~/soft-ai\ncommand rm -rf unpacked\ndpkg-deb -R python3-libnvinfer_8.5.3-1+cuda11.8_amd64.deb unpacked\n\n\ngrep Depends unpacked/DEBIAN/control\n\nDepends: python3 (>= 3.10), python3 (<< 3.11), libnvinfer8 (= 8.5.3-1+cuda11.8), libnvinfer-plugin8 (= 8.5.3-1+cuda11.8), libnvparsers8 (= 8.5.3-1+cuda11.8), libnvonnxparsers8 (= 8.5.3-1+cuda11.8)\n\n\n\nsed -i 's/python3 (>= 3.10), python3 (<< 3.11), /python3.10, /' unpacked/DEBIAN/control\ngrep Depends unpacked/DEBIAN/control\n\nDepends: python3.10, libnvinfer8 (= 8.5.3-1+cuda11.8), libnvinfer-plugin8 (= 8.5.3-1+cuda11.8), libnvparsers8 (= 8.5.3-1+cuda11.8), libnvonnxparsers8 (= 8.5.3-1+cuda11.8)\n\n\n\ndpkg-deb -b unpacked python3-libnvinfer_8.5.3-1+cuda11.8_amd64_fixed.deb\n\ndpkg-deb: building package 'python3-libnvinfer' in 'python3-libnvinfer_8.5.3-1+cuda11.8_amd64_fixed.deb'.\n\n\n\nsudo dpkg -i ./python3-libnvinfer_8.5.3-1+cuda11.8_amd64_fixed.deb ./python3-libnvinfer-dev_8.5.3-1+cuda11.8_amd64.deb\n\nSelecting previously unselected package python3-libnvinfer.\n(Reading database ... 1157567 files and directories currently installed.)\nPreparing to unpack .../python3-libnvinfer_8.5.3-1+cuda11.8_amd64_fixed.deb ...\nUnpacking python3-libnvinfer (8.5.3-1+cuda11.8) ...\nSelecting previously unselected package python3-libnvinfer-dev.\nPreparing to unpack .../python3-libnvinfer-dev_8.5.3-1+cuda11.8_amd64.deb ...\nUnpacking python3-libnvinfer-dev (8.5.3-1+cuda11.8) ...\nSetting up python3-libnvinfer (8.5.3-1+cuda11.8) ...\nSetting up python3-libnvinfer-dev (8.5.3-1+cuda11.8) ...\n\n\n\nsudo apt -qq install tensorrt\n\ntensorrt is already the newest version (8.5.3.1-1+cuda11.8).\n0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n\n\n\n\nGlossary\n\n\nNVIDIA TensorRT: An inference accelerator for deep learning models that optimizes and deploys trained neural networks for inferencing on NVIDIA GPUs.\npython3-libnvinfer package: A package required to use TensorRT but has a dependency issue with the system’s Python version.\nDependency issue: A problem that arises when a software package requires certain libraries or packages to run, and those libraries or packages are either not installed or not compatible with the system.\ndpkg command: A command used to install Debian packages."
  },
  {
    "objectID": "posts/setup/setup.html#install-rust",
    "href": "posts/setup/setup.html#install-rust",
    "title": "My setup for AI work on my Debian PC",
    "section": "Install Rust",
    "text": "Install Rust\nSome Python modules now depend on Rust to build. Also, I wanted to build Anki from source. I decided to install Rust system-wide, in /opt/rust:\n\ncd /tmp\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs > install_rust.sh\nsudo RUSTUP_HOME=/opt/rust CARGO_HOME=/opt/rust sh ./install_rust.sh -y --no-modify-path\n\n\n. /opt/rust/env\nrustup default stable\n\n\n\nGlossary\n\n\nRust: A systems programming language that is known for its speed, reliability, and memory safety features. It is often used for developing web browsers, operating systems, and game engines, and has gained popularity in the field of machine learning for its ease of integration with Python.\nrustup: A command-line tool for managing Rust installations and its various components, such as different toolchains and target platforms.\nCARGO_HOME: An environment variable used by Rust to specify the directory where Cargo, the package manager for Rust, stores its configuration and package cache.\nRUSTUP_HOME: An environment variable used by Rust to specify the directory where rustup stores its configuration and installed toolchains.\nAnki: a popular open-source flashcard application designed to help users learn and memorize information efficiently. It allows users to create digital flashcards containing text, images, audio, and video, and use various study techniques such as spaced repetition to optimize learning and retention. Anki is available for Windows, macOS, Linux, Android, and iOS platforms."
  },
  {
    "objectID": "posts/setup/setup.html#python-environments",
    "href": "posts/setup/setup.html#python-environments",
    "title": "My setup for AI work on my Debian PC",
    "section": "Python environments",
    "text": "Python environments\nI’m going to add Python venvs under /opt/venvs, and use hard linking to share large files between them.\n\n\n\n\n\n\n\n\n\nFolder\nDepends\nUses\nPurpose\n\n\n\n\npython3.10-ai\nDebian’s python3.10\ntorch stable\nAI development\n\n\npython3.10-webui\npython3.10-ai\ntorch stable\nstable-diffusion-webui\n\n\n\n\nThe base venv python3.10-ai\nThis venv is a base environment for AI development, containing various packages and libraries useful for working with deep learning models and related tasks. These include popular libraries like fastai, PyTorch, TensorFlow, scikit-learn, and NumPy, as well as some more specialized tools. The different packages are described in the glossary for this section.\n\nmkdir -p /opt/venvs\ncd /opt/venvs\n\n\nmkdir -p python3.10-ai\npython3.10 -m venv python3.10-ai/venv\n\n\nprintf \"%s\\n\" torch pipdeptree torchvision torchaudio tensorflow \\\n    jupyter jupyterlab ipywidgets bash_kernel jupyter-c-kernel nbdev fastai \\\n    pandas matplotlib scipy scikit-learn scikit-image gradio onnx \\\n    huggingface_hub transformers diffusers accelerate timm safetensors \\\n    numba fastbook > python3.10-ai/require.txt\n\n\n(\nset -e\n. /opt/venvs/python3.10-ai/venv/bin/activate\npip install -qq -U -r python3.10-ai/require.txt\npython -m bash_kernel.install\ninstall_c_kernel\n)\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nopen-clip-torch 2.15.0 requires protobuf==3.20.*, but you have protobuf 3.19.6 which is incompatible.\nInstalling IPython kernel spec\nInstalling IPython kernel spec\n/opt/venvs/python3.10-ai/venv/bin/install_c_kernel:32: DeprecationWarning: replace is ignored. Installing a kernelspec always replaces an existing installation\n  KernelSpecManager().install_kernel_spec(td, 'c', user=user, replace=True, prefix=prefix)\n\n\nUpgrade to the latest ipywidgets. This supposedly conflicts with fastbook, but is needed for Jupyter Lab.\n\n(\nset -e\n. /opt/venvs/python3.10-ai/venv/bin/activate\npip install -qq -U ipywidgets\njupyter nbextension enable --py --sys-prefix widgetsnbextension\n)\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastbook 0.0.29 requires ipywidgets<8, but you have ipywidgets 8.0.4 which is incompatible.\nEnabling notebook extension jupyter-js-widgets/extension...\n      - Validating: OK\n\n\nThe pydoc command doesn’t seem to be installed properly in venvs, so I added it:\n\ncat <<'END' > python3.10-ai/venv/bin/pydoc\n#!/bin/sh\npython -m pydoc \"$@\"\nEND\n\nchmod +x python3.10-ai/venv/bin/pydoc\n\n\n\nGlossary\n\n\nPython venvs: Virtual environments created by the Python venv module that allow users to create isolated Python environments with their own packages, versions, and configurations.\nhard linking: A method of creating a new file that shares the same content as an existing file without duplicating it, saving storage space and reducing the time needed to create a copy.\ntorch: PyTorch, an open-source machine learning framework for building and training neural networks.\ntorchvision: A package that provides access to popular datasets, model architectures, and image transformations for PyTorch.\ntorchaudio: A package that provides audio processing functionalities for PyTorch, such as loading and decoding audio files, applying transforms, and computing spectrograms.\ntensorflow: An open-source machine learning framework developed by Google for building and training neural networks.\njupyter: Jupyter Notebook, an open-source web application that allows users to create and share documents containing live code, equations, visualizations, and narrative text.\njupyterlab: The next-generation web-based user interface for Jupyter Notebook, featuring a more modern and flexible interface, multiple panes, and support for Jupyter extensions.\nipywidgets: A library that provides interactive HTML widgets for Jupyter Notebook and JupyterLab, enabling users to create sliders, dropdowns, buttons, and other graphical controls that can be used to modify code outputs and visualizations.\nbash_kernel: A Jupyter kernel that allows users to run Bash commands and scripts in Jupyter Notebook and JupyterLab.\njupyter-c-kernel: A Jupyter kernel that allows users to run C code in Jupyter Notebook and JupyterLab.\nnbdev: A library that allows users to create Python modules from Jupyter Notebooks, making it easier to develop, test, and publish code.\nfastai: An open-source library built on top of PyTorch that provides high-level abstractions for training and deploying machine learning models, including computer vision, natural language processing, and tabular data analysis.\npandas: A data analysis library for Python that provides powerful data structures and tools for manipulating and analyzing data.\nmatplotlib: A plotting library for Python that provides a variety of visualizations, including line plots, scatter plots, bar charts, histograms, and more.\nscipy: A library for scientific computing in Python that provides a wide range of mathematical algorithms, including optimization, integration, interpolation, signal processing, and more.\nscikit-learn: A machine learning library for Python that provides a variety of supervised and unsupervised learning algorithms, including regression, classification, clustering, and dimensionality reduction.\nscikit-image: An image processing library for Python that provides a variety of algorithms for image enhancement, filtering, segmentation, and feature extraction.\ngradio: An open-source library that allows users to quickly create custom web interfaces for machine learning models, enabling users to interact with models using sliders, dropdowns, text boxes, and other controls.\nonnx: Open Neural Network Exchange, an open-source format for representing machine learning models that allows interoperability between different frameworks and platforms.\nhuggingface_hub: A library that provides access to a wide range of pre-trained machine learning models for natural language processing, computer vision, and other tasks, hosted on the Hugging Face Hub.\ntransformers: A library that provides state-of-the-art natural language processing models for tasks such as sentiment analysis, question answering, and language translation, based on transformer architectures.\ndiffusers: A library that provides a set of PyTorch modules for training diffusion models, a type of probabilistic generative model that can be used for tasks such as image synthesis, denoising, and inpainting.\npipdeptree: a command-line utility that displays the installed Python packages in the form of a dependency tree\nAccelerate: a library that enables PyTorch code to run across any distributed configuration with just four lines of code\nPyTorch Image Models (timm): a deep-learning library that includes a collection of state-of-the-art computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations, and training/validating scripts with the ability to reproduce ImageNet training results\nSafetensors: a repository that implements a new simple format for storing tensors safely and efficiently, instead of using pickle\nNumba: a high-performance Python compiler that translates Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library; it offers a range of options for parallelizing Python code for CPUs and GPUs, often with only minor code changes\nfastbook: the Fast.ai book published as Jupyter Notebooks, that covers deep learning using fastai and PyTorch\n\n\n\n\nThe secondary venv python3.10-webui\nThis is a secondary Python venv called python3.10-webui. It will be used to support the stable-diffusion web user interface, and is based on the python3.10-ai venv. Hard linking will be used to share large files between the two venvs.\n\ncd /opt/venvs\n\n\nmkdir -p python3.10-webui\n\n\ncommand rm -rf python3.10-webui/venv\n\n\ncp -al python3.10-ai/venv python3.10-webui/venv\n\n\nyes | venv_move python3.10-webui/venv\n\n\n(\ncat python3.10-ai/require.txt ~/soft-ai/stable-diffusion-webui/requirements.txt \necho fastapi==0.90.1\n) > python3.10-webui/require.txt\n\n\n(\nset -e\n. python3.10-webui/venv/bin/activate\npip install -qq -U -r python3.10-webui/require.txt\n)\n\n\nln -f python3.10-ai/venv/bin/pydoc python3.10-webui/venv/bin/pydoc\n\n\n\nGlossary\n\n\nhard linking: a file system feature that allows multiple files to share the same physical storage location on disk. Hard linking a file creates a new file that points to the same location as the original file. This can be used to save disk space and reduce redundancy in a file system.\nvenv_move: this is a Bash script that is used to move a Python virtual environment (venv) to a new location. The script takes one argument, which is the path to the venv directory that needs to be moved.\n\n\n\n\nstable-diffusion-webui\nIn this section, we will install the automatic1111 stable-diffusion-webui app and set up custom scripts to update and launch it. The stable-diffusion-webui app is a web user interface for the Stable Diffusion model, which is a deep learning model for image classification. The update script will allow us to easily update the app when new changes are pushed to the Git repository. The launch script will activate the correct virtual environment and launch the web user interface.\nThe app works without these custom scripts, but I wanted to allow for careful updates and custom launch options.\n\ncd ~/soft-ai\n[ -d stable-diffusion-webui ] ||\ngit clone git@github.com:AUTOMATIC1111/stable-diffusion-webui.git\ncd stable-diffusion-webui\n\n\nmy update script\n\ncat update\n\n#!/bin/sh\nset -e\n. /opt/venvs/python3.10-webui/venv/bin/activate\ngit stash\ngit pull\ngit stash pop || true\npip install -r requirements.txt\n\n\n\n\nmy launch script\n\ncat launch\n\n#!/bin/bash\nset -ae\n. /opt/venvs/python3.10-webui/venv/bin/activate\ncd \"$(dirname \"$(readlink -f \"$0\")\")\"\nSAFETENSORS_FAST_GPU=1\nCOMMANDLINE_ARGS=\"--xformers --api $*\"  # --no-half-vae\nREQS_FILE=\"requirements.txt\"\npython launch.py\n\n\nNext, I launch the webui to install some extra requirements, and check that it works.\n\n\nGlossary\n\n\nstable-diffusion-webui: A web user interface for the Stable Diffusion model, a deep learning model for image classification. The stable-diffusion-webui app is installed in this section, along with custom scripts for updating and launching it.\nupdate script: A custom script for updating the stable-diffusion-webui app. The script activates the correct virtual environment and pulls changes from the Git repository, installs any new requirements, and restarts the app.\nlaunch script: A custom script for launching the stable-diffusion-webui app. The script activates the correct virtual environment and sets some environment variables before running the launch.py script.\nSAFETENSORS_FAST_GPU=1: an environment variable used with the Safetensors library in deep learning applications. Normally, models are loaded to the CPU and then moved to the GPU, which can involve an additional memory copy step. The “SAFETENSORS_FAST_GPU=1” option allows models to be loaded directly onto the GPU, bypassing the memory copy step and potentially improving performance. However, this option is untested and may not be suitable for all applications.\nCOMMANDLINE_ARGS: An environment variable used in the launch script to pass command line arguments to the launch.py script.\nREQS_FILE: An environment variable used in the launch script to specify which requirements file should be used for the stable-diffusion-webui app.\nrequirements.txt file: a text file that lists the dependencies of a Python project. It contains a list of package names and optional version numbers that are required for the project to run. This file can be used by the pip package installer to automatically install all required packages and their dependencies, like this: pip install -r requirements.txt\n\n\n\n\n\nCompare the two venvs\nIn this section, we compare the packages installed in the two Python virtual environments we created earlier. By running the “pip freeze” command in each venv and storing the output in a text file, we can then compare the contents of those files using the “comm” command. The resulting output shows the packages that are installed in one venv but not the other. We can use this information to ensure that both venvs have the necessary packages for our projects.\nRunning pip check in both venvs can help ensure that all packages are installed correctly and functioning properly. In the output, we see a package conflict related to the version of protobuf installed, but this does not seem to cause any issues in practice as the app still works.\n\ncd /opt/venvs\n\n\n(\n. python3.10-ai/venv/bin/activate\npip freeze | grep -v '^[#-]' | sort > python3.10-ai/freeze.txt\npip check\n)\n\nNo broken requirements found.\n\n\n\n(\n. python3.10-webui/venv/bin/activate\npip freeze | grep -v '^[#-]' | sort > python3.10-webui/freeze.txt\npip check\n)\n\ntensorflow 2.11.0 has requirement protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0.\n\n\n: 1\n\n\n\ncomm -3 python3.10-ai/freeze.txt python3.10-webui/freeze.txt | tee comm.txt\n\n\n\nGlossary\n\npip freeze: A command used to output the names and versions of installed Python packages in the format required for a requirements.txt file. We use pip freeze to generate the requirements.txt files for both venvs so we can compare them.\npip check: A command in pip that checks the consistency of installed packages, verifying that all dependencies are met and all files are intact. It will report any issues or conflicts with installed packages, and can help identify packages that need to be updated or removed.\nsort: A Unix utility used to sort the lines of a file alphabetically. We use the sort command to ensure that the lines in the requirements.txt files are in the same order, making it easier to compare them. The comm utility requires that it’s inputs are sorted.\ncomm: A Unix utility used to compare two files line by line. In this section, we use the comm command to compare the contents of two requirements.txt files and display the lines that are unique to each file.\n\n\n\nCheck disk usage and savings\nIn this section, we use the du tool to check the disk usage and savings of the base venv and webui venv. By using hard links, we can see that we are saving nearly 6GB of disk space. We use the du -sh command to show the size of each venv separately and du -csh command to show the total size of both venvs.\n\ndu -sh ./python3.10-ai\ndu -sh ./python3.10-webui\n\n5.9G    ./python3.10-ai\n6.2G    ./python3.10-webui\n\n\n\ndu -csh ./python3.10-{ai,webui}\n\n5.9G    ./python3.10-ai\n652M    ./python3.10-webui\n6.6G    total\n\n\n\n\nGlossary\n\n\ndu: A command-line tool used to estimate file space usage in a file system. It can display the file size in human-readable format.\ndisk usage: The amount of disk space occupied by a file or directory in a file system.\nhard links: A feature of the file system that allows multiple files to share the same data blocks on a storage device. Hard links allow a file to have multiple names in different directories or in the same directory.\n\n\n\n\nSave the setup in a git repo\nIn this section, we initialize a new Git repository to save our virtual environment setup. We create a “.gitignore” file to exclude the venvs from version control, add all files to the Git staging area, and commit the changes with a message. This allows us to easily track and version our venv setup and changes over time.\n\ngit init\necho venv > .gitignore\ngit add -A\ngit commit -m 'venvs'\n\nReinitialized existing shared Git repository in /opt/venvs/.git/\n[main 8fd5a24] venvs\n 2 files changed, 15 insertions(+), 1 deletion(-)\n\n\n\n\nGlossary\n\n\nGit: A version control system used for tracking changes to files and collaborating on projects.\nRepository: A central location in which data is stored and managed.\n.gitignore: A file used to exclude files and directories from being tracked by Git.\nStaging area: A place where files are stored before they are committed to the repository.\nCommit: A permanent record of changes to files in the repository, along with a message that describes the changes."
  },
  {
    "objectID": "posts/setup/setup.html#building-xformers",
    "href": "posts/setup/setup.html#building-xformers",
    "title": "My setup for AI work on my Debian PC",
    "section": "Building xformers",
    "text": "Building xformers\nIf you need to build and install xformers from source, this is how to do it. It took nearly half an hour on a fast computer. I think that it didn’t build things in parallel. I used this with stable-diffusion-webui, when the binary packages of xformers weren’t working.\n\ncd ~/soft-ai\n[ -d xformers ] || git clone https://github.com/facebookresearch/xformers.git\ncd xformers\n\n\ngit submodule update --init --recursive\n\n\ngit pull\ngit submodule update --recursive\n\nAlready up to date.\n\n\n\npython setup.py clean --all\n\nrunning clean\n'build/lib.linux-x86_64-cpython-310' does not exist -- can't clean it\n'build/bdist.linux-x86_64' does not exist -- can't clean it\n'build/scripts-3.10' does not exist -- can't clean it\n\n\n\necho $VIRTUAL_ENV\n\n/opt/venvs/python3.10-ai/venv\n\n\n\nCUDA_HOME=\"/usr/local/cuda-11.7\" CC=gcc-11 CXX=g++-11 MAKEFLAGS=\"-j$(nproc)\" \\\ntime pip install -e . 2>&1 | tee build.log\n\nObtaining file:///home/sam/soft-ai/xformers\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nRequirement already satisfied: numpy in /opt/venvs/python3.10-ai/venv/lib/python3.10/site-packages (from xformers==0.0.17+b89a493.d20230303) (1.23.5)\nRequirement already satisfied: pyre-extensions==0.0.23 in /opt/venvs/python3.10-ai/venv/lib/python3.10/site-packages (from xformers==0.0.17+b89a493.d20230303) (0.0.23)\nRequirement already satisfied: torch>=1.12 in /opt/venvs/python3.10-ai/venv/lib/python3.10/site-packages (from xformers==0.0.17+b89a493.d20230303) (1.13.1)\nRequirement already satisfied: typing-extensions in /opt/venvs/python3.10-ai/venv/lib/python3.10/site-packages (from pyre-extensions==0.0.23->xformers==0.0.17+b89a493.d20230303) (4.5.0)\nRequirement already satisfied: typing-inspect in /opt/venvs/python3.10-ai/venv/lib/python3.10/site-packages (from pyre-extensions==0.0.23->xformers==0.0.17+b89a493.d20230303) (0.8.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/venvs/python3.10-ai/venv/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.17+b89a493.d20230303) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/venvs/python3.10-ai/venv/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.17+b89a493.d20230303) (8.5.0.96)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/venvs/python3.10-ai/venv/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.17+b89a493.d20230303) (11.7.99)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/venvs/python3.10-ai/venv/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.17+b89a493.d20230303) (11.10.3.66)\nRequirement already satisfied: setuptools in /opt/venvs/python3.10-ai/venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.12->xformers==0.0.17+b89a493.d20230303) (66.1.1)\nRequirement already satisfied: wheel in /opt/venvs/python3.10-ai/venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.12->xformers==0.0.17+b89a493.d20230303) (0.38.4)\nRequirement already satisfied: mypy_extensions>=0.3.0 in /opt/venvs/python3.10-ai/venv/lib/python3.10/site-packages (from typing-inspect->pyre-extensions==0.0.23->xformers==0.0.17+b89a493.d20230303) (1.0.0)\nInstalling collected packages: xformers\n  Running setup.py develop for xformers\nSuccessfully installed xformers-0.0.17+b89a493.d20230303\n1498.23user 47.47system 21:29.43elapsed 119%CPU (0avgtext+0avgdata 1777728maxresident)k\n4848inputs+12701968outputs (64major+40118051minor)pagefaults 0swaps\n\n\n\n\nGlossary\n\n\nxformers: a library for accelerating transformer-based models in PyTorch.\nCUDA_HOME: an environment variable that specifies the path to the CUDA installation directory.\ngcc-11 and g++-11: the GNU Compiler Collection version 11 for the C and C++ programming languages, respectively. Xformers won’t build with gcc-12, so we need to specify the older version.\nnproc: a command that outputs the number of processing units available to the current process.\nsetup.py: a Python script that is used to package and distribute Python modules.\nsubmodule: a feature in Git that allows a repository to contain another repository as a subdirectory.\ntee: a command that reads standard input and writes it to both standard output and one or more files.\ntime: a command that displays the system time for a command to execute."
  },
  {
    "objectID": "posts/deadends/deadends.html",
    "href": "posts/deadends/deadends.html",
    "title": "Graveyard of bad setup ideas",
    "section": "",
    "text": "Here is a list of other things I tried for my Debian AI setup, which didn’t work out so well. They didn’t cause disasters for me, but they were dead-ends; they didn’t solve my setup problems. Here be dragons!"
  },
  {
    "objectID": "posts/deadends/deadends.html#allow-myself-to-install-packages-to-usrlocal",
    "href": "posts/deadends/deadends.html#allow-myself-to-install-packages-to-usrlocal",
    "title": "Graveyard of bad setup ideas",
    "section": "Allow myself to install packages to /usr/local",
    "text": "Allow myself to install packages to /usr/local\nSince I wrote this section, Debian has modified their pip so that it refuses to install to /usr/local, unless we use the --break-system-packages flag.\nIt’s not safe to run pip install -U as root. It will merrily remove files from Debian-packaged Python modules under /usr/lib, and mess up the system. To avoid this, I changed permissions so that my regular user account can write to everything under /usr/local via the staff group. I do the same for /opt because I want to upgrade Rust from my own user ID also:\n\nsudo adduser $USER staff\nsudo chgrp -R staff /usr/local /opt\nsudo chmod -R g+w /usr/local /opt\n\nThis is somewhat of a security concern, but if a bad guy gets a local shell it’s pretty much game over anyway. To make it safer, I could have used a different non-root account.\nIn order to upgrade a pip package where the same package was also installed in /usr/lib by dpkg, I need to use pip -I -U packagename. The -I flag tells it to ignore installed packages, i.e. don’t try to remove them. I only do that as needed for individual packages, when a normal upgrade fails.\nThis setup might upset Debian-packaged programs that depend on older versions of the Python libraries that I’ve upgraded. I haven’t noticed any problems like that yet.\nIf you try to downgrade a package, and if multiple versions of a package are accidentally installed under /usr/local/lib/python3.10 at that same time, it can get confused, so watch out for that. You might need to manually remove one of the versions.\n\n\nGlossary\n\n\npip: A package manager for Python packages that allows users to easily install, upgrade, and remove Python packages.\n/usr/local: A directory on Unix-like systems where locally installed software is usually placed, separate from the operating system’s own packages.\nDebian: A popular Linux distribution known for its stability and ease of use.\n–break-system-packages: A flag used with pip to force installation of packages that conflict with system packages, even though this can cause problems.\nroot: The default administrative user in Unix-like operating systems, with full permissions to all system files and directories.\npermissions: The settings that control who can access and modify files and directories on a computer.\nstaff group: A group on Unix-like systems that provides additional permissions to users who belong to it.\nchgrp: A Unix command used to change the group ownership of a file or directory.\nchmod: A Unix command used to change the permissions of a file or directory.\nsecurity concern: A potential risk to the security of a system, data, or user privacy.\ndpkg: A package manager for Debian-based Linux distributions.\nupgrade: The process of installing a newer version of a software package over an existing version to get new features or bug fixes.\ndowngrade: The process of installing an older version of a software package over a newer version."
  },
  {
    "objectID": "posts/deadends/deadends.html#enable-switching-the-system-python-version",
    "href": "posts/deadends/deadends.html#enable-switching-the-system-python-version",
    "title": "Graveyard of bad setup ideas",
    "section": "Enable switching the system python version",
    "text": "Enable switching the system python version\nThe default Python version for Debian “testing” at time of writing is Python 3.11, however we don’t have a stable release of pytorch for Python 3.11 yet.\n\npython3.11 -m pip install --break-system-packages torchvision\n\nERROR: Could not find a version that satisfies the requirement torchvision (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3)\nERROR: No matching distribution found for torchvision\n\n\n\n: 1\n\n\nI decided to try switch the system Python version back to 3.10. This is not recommended, but at least it’s easy to undo!\nAnyway, let’s enable switching the system Python version back to 3.10, using update-alternatives.\nFirst, note that we can undo this change later, if necessary, as follows:\n\nsudo update-alternatives --remove-all python3\nsudo update-alternatives --remove-all pydoc3\nsudo ln -sf python3.11 /usr/bin/python3\nsudo ln -sf pydoc3.11 /usr/bin/pydoc3\n\nWe can set up alternatives for python3:\n\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 11\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 10\n# sudo update-alternatives --set python3 /usr/bin/python3.10\n\nand do the same for pydoc3:\n\nsudo update-alternatives --install /usr/bin/pydoc3 pydoc3 /usr/bin/pydoc3.11 11\nsudo update-alternatives --install /usr/bin/pydoc3 pydoc3 /usr/bin/pydoc3.10 10\n# sudo update-alternatives --set pydoc3 /usr/bin/pydoc3.10\n\n\nupdate-alternatives --list python3\n\n\nupdate-alternatives --list pydoc3\n\n\n\nGlossary\n\n\ntorchvision: a PyTorch library containing popular datasets, model architectures, and common image transformations for computer vision.\nupdate-alternatives: a Debian/Ubuntu tool for managing alternative versions of executables, such as Python or Pydoc.\nsystem Python: the version of Python installed on a Debian/Ubuntu system and used by default for system-level processes.\nbreak-system-packages: a pip option that allows installation of packages that may break other system packages.\nln -s: a command for creating symbolic links between files.\npydoc: a Python documentation tool."
  },
  {
    "objectID": "posts/deadends/deadends.html#install-nightly-torch-with-python-3.11",
    "href": "posts/deadends/deadends.html#install-nightly-torch-with-python-3.11",
    "title": "Graveyard of bad setup ideas",
    "section": "Install nightly torch with Python 3.11",
    "text": "Install nightly torch with Python 3.11\n\npip install -qq -U fastbook jupyter jupyterlab tensorflow ipywidgets\n\nAt some point I saw a warning about compatibility between fastai and torch.\nfastai 2.7.11 has requirement torch<1.14,>=1.7, but you have torch 2.0.0.dev20230220+cu117\nI decided to try using it with nightly torch anyway:\n\npip install -qq -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu117\n\nExtra packages: nbdev, and kernels for bash and C:\n\npip install -qq -U nbdev bash_kernel jupyter-c-kernel"
  },
  {
    "objectID": "posts/deadends/deadends.html#use-the-old-pip-resolver-if-necessary",
    "href": "posts/deadends/deadends.html#use-the-old-pip-resolver-if-necessary",
    "title": "Graveyard of bad setup ideas",
    "section": "Use the old pip resolver, if necessary",
    "text": "Use the old pip resolver, if necessary\nAs of pip 20.3, a new resolver has been introduced, which doesn’t always work. As of pip 21.0 the old working resolver is unsupported and slated for removal dependent on pip team resources.\n\npip --version\n\npip 23.0 from /usr/lib/python3/dist-packages/pip (python 3.11)\n\n\nWith this version of pip, it’s possible to use the “legacy resolver” with the following option:\n\npip install -U -r requirements.txt --use-deprecated=legacy-resolver\n\nOr, I can downgrade pip to version 20.2.4, before they switched to the new resolver.\nRefer to https://stackoverflow.com/a/67408694 for more info.\n\npip install pip==20.2.4\n\n\n\nGlossary\n\n\nresolver: Part of pip responsible for resolving package dependencies and deciding which versions to install.\nlegacy resolver: The older version of pip’s resolver algorithm, which is still supported in some versions of pip but has been deprecated and is no longer the default."
  },
  {
    "objectID": "posts/deadends/deadends.html#pypi-does-not-include-onnxruntime-for-python3.11",
    "href": "posts/deadends/deadends.html#pypi-does-not-include-onnxruntime-for-python3.11",
    "title": "Graveyard of bad setup ideas",
    "section": "PyPI does not include onnxruntime for python3.11",
    "text": "PyPI does not include onnxruntime for python3.11\nAt one point I was hoping to use python3.11 rather than python3.10. I attempted to build onnxruntime from source, but didn’t persevere with it.\n\n\nGlossary\n\n\nPyPI: PyPI (Python Package Index) is a repository of software for the Python programming language. It contains packages of software developed by various authors that are published and shared with the wider Python community.\nonnxruntime: onnxruntime is an open-source engine for executing machine learning models that are defined in the Open Neural Network Exchange (ONNX) format. It provides high performance and supports multiple platforms, including CPU, GPU, and FPGA.\nSource code is the human-readable code that programmers write to create computer programs. It needs to be compiled or interpreted to run on a computer.\nBuild: Building refers to the process of compiling or otherwise preparing source code for execution on a specific platform or architecture.\nOpen Neural Network Exchange (ONNX): ONNX is an open-source format for representing machine learning models. It is designed to be interoperable across different frameworks and platforms, allowing models to be easily transferred and executed on different hardware.\nFPGA: FPGA stands for Field-Programmable Gate Array. It is an integrated circuit that can be programmed to perform a wide range of computing tasks, including machine learning inference."
  },
  {
    "objectID": "posts/deadends/deadends.html#untold-difficulties",
    "href": "posts/deadends/deadends.html#untold-difficulties",
    "title": "Graveyard of bad setup ideas",
    "section": "Untold difficulties! 😭",
    "text": "Untold difficulties! 😭\nI haven’t described all of the various problems I had to deal with, due to trying various hacky approaches rather than venvs. There were plenty of problems! I guess I learned something from the experience, anyway. I wish Lambda Stack worked on Debian, not only Ubuntu, it would have been a lot easier than this!\n\n\nGlossary\n\n\nhacky approaches: Informal or unconventional methods used to solve a problem or complete a task, often using shortcuts or workarounds instead of following established best practices.\nLambda Stack: A software stack from Lambda Labs that provides pre-built binaries of deep learning frameworks and other software packages for Ubuntu-based systems."
  }
]