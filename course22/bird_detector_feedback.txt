I'm going over the part 1 notebooks carefully again, and I want to give some feedback that in my opinion, the bird detector example is not very good.

I preferred the old example of distinguishing cat and dog breeds, because it was very impressive (being something that I cannot do myself) and it functioned correctly. i.e. under the assumption that the image is a photo of a dog or cat, it did a great job of predicting the breed.  The simpler dog vs cat example and the black / grizzly / teddy bears example were okay too, although less impressive than the cat and dog breeds.

The bird detector on the other hand according to the XKCD cartoon should "check whether the photo is a bird". Although this seems simpler, I think this is a much more difficult task, as we have not much limited the domain of possible inputs. Our detector should accept any sort of photo, and tell us whether it is a photo of a bird or not.

The network trained in the example notebook does not distinguish photos of birds vs other photos very well at all, so I think it's not a good example.

In the demo the bird detector is only tested on a single image, i.e. the `bird.jpg` that was downloaded originally, which is likely included in the training set anyway.

I decided to test it on another image, a picture of a cow, which it told me was a bird. Not a good sign!

I guess the model has learned to distinguish pictures without an animal from pictures with an animal, or something like that.

Looking further into this I collected more images and attempted to train the model to distinguish birds from a wide range of other random images and animals, and I found that it was not easy at all, and I didn't succeed to my satisfaction yet.

I thought that my best effort after a few hours might be good enough, so I tried it on a hundred pictures of random animals that I had collected last year. The model gave a very low rate of false negatives, but it gave me around 60% false positives i.e. it guessed that various different animals were birds, and most of the animals that it thought were birds were in fact not.

At the start of the notebook, it says: But today, we can do exactly that, in just a few minutes, using entirely free resources! and the conclusion at the end of the notebook is that: creating computer vision classification models has gone from "so hard it's a joke" to "trivially easy and free"!

But I don't think we've demonstrated that with this example. Yes, more often than not it can distinguish a picture of forest without any animals from a picture of a bird, but it cannot distinguish a cow from a bird.

I guess to make a good bird detector in 2022 would be closer to a one week project than a 5 year project, but I don't think it's trivially easy at least not if we follow this sort of method.


More notes about this notebook:

- The code had changed to download 600 of each, with sun and shade.
- Forest vs bird isn't sufficient to check if something is a bird. e.g. a photo of a cow that I tested is predicted to be a bird.
- At first I changed it to search for landscape, forest, animal, and just plain "photo".
- Keep it simple, search for `bird photo` and `-bird photo`.
- Got confused with `penguin`, `butterfly`.
- The original `bird.jpg` and `forest.jpg` will likely appear in the training set or validation set, need to remove them to test it properly.
- Should remove duplicates, to avoid contaminating the validation set with images from training, and to avoid overfitting. I will use the tools `fdupes`, `findimagedupes`, and `qiv`.
- When removing duplicates, we can also remove knows birds that got into the `not_bird` set.
- `resize_images` may fail if we do it before `verify_images`, in which case we might not finish all the searches.
- `resize_images` doesn't work for files with unusual extensions. We should correct extensions first, or just remove those with unknown extensions.
- I tried hard to train it to handle more edge cases.
- I tested my "bird detector" on a large collection of animal photos and others photos. From a sample of 100 photos, it detected 39 as birds, however only 16 of those are actually photos of birds. So it had a nearly 60% false positive rate. The false negative rate was lower, close to zero.
- I would like to keep trying to improve the "bird detector", but for now, in conclusion, it seems to me that it is not a trivial task to train an ANN to reliably distinguish photos of birds from other images.


Practical Deep Learning for Coders is an awesome course, and I'm very grateful to have access to various editions of the course material. I'm having a look at the 2022 part 1 notebooks at the moment, and I want to give the respectful feedback that I don't like the "bird detector" example.

The notebook shows an XKCD cartoon, posing the problem to "check whether the photo is of a bird". The notebook claims that "today, we can do exactly that, in just a few minutes..."

But the model we end up with does not come close to achieving this goal. For example, I tried it on a photo of a cow, and it told me that it is likely a bird. This makes some sense, as a cow is more similar to a bird than a forest, and our model has only been fine tuned on birds and forests.

I spent a few hours trying to train the model on a wider range images, and achieved some improvement, but then I tried it on a set of 100 random animal photos and found that it is still not very reliable at all, giving around 60% false positives. My next step would be to include a large set of different animal photos in the training data, but that's too much for an introductory example.

I don't know if there is an easy way to solve the problem of "bird detector" in 2022 that would be suitable for the first lesson. If so, I'd like to hear about it. I don't think it would be easy to fix the model to work reliably, especially if we try to cover tricky cases such as butterflies, penguins, bats, and flying fish.

I suspect that it would be much more difficult to distinguish "a photo of a bird" from "any other photo", compared to the early examples in previous versions of the course.

I like the examples from older courses: dog and cat breeds, dog vs cat, and teddy / black / grizzly bears. Each of those examples has a limited domain of acceptable inputs, and works well at doing what it sets out to do. The old cat and dog breeds example is my favourite, because it does something impressive that I can't do myself.
